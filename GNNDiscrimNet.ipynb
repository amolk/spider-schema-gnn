{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERTRand-DR\n",
    "```\n",
    "Input: NLU + Predicted Columns\n",
    "Output: 1 if correct, 0 if incorrect\n",
    "Side effect: Reranking of BEAM outputs from GNN model\n",
    "Architecture:\n",
    "BERT + classification layer\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The beam output files are generated with the following command: (beam output should be 40 instead of 10 for train, can be 10 for dev). Make sure to modify the json source file (train, dev etc) as needed\n",
    "```\n",
    "allennlp predict experiments/bert-spider-low-lr/ ./datasets/spider/train_spider.json --predictor spider_discriminator --use-dataset-reader --cuda-device=0 --silent --output-file out.jsonlines --include-package models.semantic_parsing.spider_parser --include-package dataset_readers.spider --include-package predictors.discriminator_dataset_generator --weights-file experiments/bert-spider-low-lr/best.th\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-28T04:41:41.524612Z",
     "start_time": "2019-09-28T04:41:16.257342Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!conda install -y -c conda-forge ipywidgets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -y line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-28T04:39:10.750532Z",
     "start_time": "2019-09-28T04:39:10.744807Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import *\n",
    "import numpy as np\n",
    "import sql_metadata\n",
    "from pathlib import Path\n",
    "import json\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import pprint\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-28T04:39:11.339144Z",
     "start_time": "2019-09-28T04:39:10.932325Z"
    }
   },
   "outputs": [],
   "source": [
    "pp = pprint.PrettyPrinter(indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-28T04:39:12.715991Z",
     "start_time": "2019-09-28T04:39:12.702989Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_beam_outputs(filename):\n",
    "    filename = Path(filename)\n",
    "    data = []\n",
    "    with filename.open('r') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "    return data\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_items = load_beam_outputs('dev_all_beam.jsonlines')\n",
    "train_items = load_beam_outputs('train_all_beam.jsonlines')  # train_all = train_spider + train_others\n",
    "val_normaleval_items = load_beam_outputs('dev_all_normaleval_beam.jsonlines') # Created with the easier evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_items[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "count_acc = 0\n",
    "count_too_many_targets = 0\n",
    "for item in val_items:\n",
    "    instances = item['instances']\n",
    "    if len(list(filter(lambda instance: instance['target'] > 0.5, instances))) == 0:\n",
    "        count +=1\n",
    "    if len(list(filter(lambda instance: instance['target'] > 0.5, instances))) > 1:\n",
    "        count_too_many_targets += 1\n",
    "    if instances[0]['target'] > 0.5:\n",
    "        count_acc += 1\n",
    "len(val_items), count, count_acc, count_too_many_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates the input for BERT:\n",
    "# Returns the tokenized bert input [CLS] <tokenized nlu> [SEP] col1 [SEP] col2 ...[SEP]\n",
    "# and the start and end locations in the tokenized input for each of the columns\n",
    "def create_tokenized_input(tokenizer, nlu, columns, tables):\n",
    "    tokens = []\n",
    "    tokens += tokenizer.tokenize('[CLS]')\n",
    "    tokens += tokenizer.tokenize(nlu)\n",
    "    tokens += tokenizer.tokenize('[SEP]')\n",
    "    column_locations = []\n",
    "    for column in columns:\n",
    "        column_str = '.'.join(column)\n",
    "        tokens += tokenizer.tokenize(column_str)\n",
    "        tokens += tokenizer.tokenize('[SEP]')\n",
    "    for table in tables:\n",
    "        tokens += tokenizer.tokenize(table)\n",
    "        tokens += tokenizer.tokenize('[SEP]')\n",
    "    return tokens, tokenizer.convert_tokens_to_ids(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-28T04:39:40.461712Z",
     "start_time": "2019-09-28T04:39:13.022667Z"
    }
   },
   "outputs": [],
   "source": [
    "# {'utterance': 'How many singers do we have?',\n",
    "# 'instances': [{'sql_query': 'select count ( * ) from singer',\n",
    "#   'tables_used': ['singer'],\n",
    "#   'columns_used': [],\n",
    "#   'target': 1.0},\n",
    "def preprocess_data(tokenizer, data, train=True):\n",
    "    new_data = []\n",
    "    for sample in tqdm(data):\n",
    "        new_sample = {}\n",
    "        new_sample['utterance'] = sample['utterance']\n",
    "        correct_instances = []\n",
    "        incorrect_instances = []\n",
    "        instances = []\n",
    "        for i, instance in enumerate(sample['instances']):\n",
    "            instance = instance.copy()\n",
    "            tokens, encoded_tokens = create_tokenized_input(tokenizer, \n",
    "                                                            sample['utterance'], \n",
    "                                                            instance['columns_used'],\n",
    "                                                            instance['tables_used'])\n",
    "            instance['tokens'] = tokens\n",
    "            instance['encoded_tokens'] = encoded_tokens\n",
    "            instance['rank'] = i\n",
    "            if len(encoded_tokens) <= 512:\n",
    "                if instance['target'] > 0.5:\n",
    "                    correct_instances.append(instance)\n",
    "                else:\n",
    "                    incorrect_instances.append(instance)\n",
    "            instances.append(instance)\n",
    "            \n",
    "            # TODO(rohan): Handle case where encoded tokens > 512, right now it's being added to instances\n",
    "        def make_key(instance): return (str(sorted(instance['columns_used'])) + str(sorted(instance['tables_used'])))\n",
    "        unique_instances = {}\n",
    "        uniq_correct_instances = []\n",
    "        for instance in correct_instances:  # Need to make sure that earlier rank ones are retained\n",
    "            key = make_key(instance)\n",
    "            if key not in unique_instances:\n",
    "                unique_instances[key] = instance\n",
    "                uniq_correct_instances.append(instance)\n",
    "        uniq_incorrect_instances = []\n",
    "        # We don't reset unique_instances because we also want to exclude incorrect instances\n",
    "        # if there is an equivalent instance in correct_instances\n",
    "        for instance in incorrect_instances:\n",
    "            key = make_key(instance)\n",
    "            if key not in unique_instances:\n",
    "                unique_instances[key] = instance\n",
    "                uniq_incorrect_instances.append(instance)\n",
    "        sample['correct_instances'] = uniq_correct_instances\n",
    "        sample['incorrect_instances'] = uniq_incorrect_instances\n",
    "        sample['instances'] = instances\n",
    "\n",
    "        # TODO(rohan): Not sure if I want this?\n",
    "        # Commenting out because I it changes the top 10 queries (eg. if there was a correct and incorrect with the same columns)\n",
    "        # sample['instances'] = sorted(sample['correct_instances'] + sample['incorrect_instances'], key=lambda ins: ins['rank'])\n",
    "        if train and len(uniq_correct_instances) == 0:  # For training, there must be at least one correct instance\n",
    "            pass\n",
    "        else:\n",
    "            new_data.append(sample)\n",
    "    return new_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_processed = preprocess_data(tokenizer, train_items, train=True)\n",
    "val_data_processed = preprocess_data(tokenizer, val_items, train=False)\n",
    "val_data_normaleval_preprocessed = preprocess_data(tokenizer, val_normaleval_items, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_data_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for s in val_data_processed:\n",
    "    if len(s['correct_instances']) > 1:\n",
    "        count += 1\n",
    "        print('---------------------')\n",
    "        print(s['utterance'])\n",
    "        for ins in s['correct_instances']:\n",
    "            print('====')\n",
    "            print(ins['sql_query'])\n",
    "            print(ins['columns_used'])\n",
    "            print(ins['tables_used'])\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data_processed[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-28T04:39:40.465346Z",
     "start_time": "2019-09-28T04:39:40.462786Z"
    }
   },
   "outputs": [],
   "source": [
    "len(train_data_processed), len(val_data_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BeamOutputDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, items):\n",
    "        self.items = items\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.items[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-28T04:39:40.484848Z",
     "start_time": "2019-09-28T04:39:40.477758Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = BeamOutputDataset(train_data_processed)\n",
    "val_dataset = BeamOutputDataset(val_data_processed)\n",
    "val_normaleval_dataset = BeamOutputDataset(val_data_normaleval_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-28T04:39:40.490904Z",
     "start_time": "2019-09-28T04:39:40.485658Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    batch_size=1,\n",
    "    dataset=train_dataset,\n",
    "    shuffle=True,\n",
    "    collate_fn=lambda x: x # So dicts aren't merged\n",
    ")\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    batch_size=1,\n",
    "    dataset=val_dataset,\n",
    "    shuffle=False,\n",
    "    collate_fn=lambda x: x\n",
    ")\n",
    "val_normaleval_loader = torch.utils.data.DataLoader(\n",
    "    batch_size=1,\n",
    "    dataset=val_normaleval_dataset,\n",
    "    shuffle=False,\n",
    "    collate_fn=lambda x: x\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-28T04:39:40.496705Z",
     "start_time": "2019-09-28T04:39:40.491719Z"
    }
   },
   "outputs": [],
   "source": [
    "train_bert = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-28T04:39:42.368505Z",
     "start_time": "2019-09-28T04:39:42.366622Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_as_batch(batch, key):\n",
    "    result = []\n",
    "    for sample in batch:\n",
    "        result.append(sample[key])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-28T04:39:42.391443Z",
     "start_time": "2019-09-28T04:39:42.386235Z"
    }
   },
   "outputs": [],
   "source": [
    "optim = torch.optim.Adam([\n",
    "    {\"params\": bert_model.parameters(), \"lr\": 5e-6}  # 5e-6 converges faster than 1e-6\n",
    "], lr=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-28T04:39:42.397353Z",
     "start_time": "2019-09-28T04:39:42.392310Z"
    }
   },
   "outputs": [],
   "source": [
    "for param in bert_model.parameters():\n",
    "    param.requires_grad = train_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_instance(bert_model, instance):\n",
    "    encoded_tokens = torch.tensor(instance['encoded_tokens']).unsqueeze(0).cuda()\n",
    "    target = int(instance['target'])\n",
    "    labels = torch.tensor([target]).unsqueeze(0).cuda()\n",
    "    outputs = bert_model(encoded_tokens, labels=labels)\n",
    "    loss1, logits = outputs[:2]\n",
    "    preds = F.softmax(logits, dim=1)\n",
    "    pred_index = torch.argmax(preds[0])  # Remove batch\n",
    "    if target == pred_index.item():\n",
    "        correct = 1\n",
    "    else:\n",
    "        correct = 0\n",
    "    return loss1, correct, preds\n",
    "\n",
    "def run_epoch(bert_model, data_loader, train_bert, optim, training=True, print_samples=False):\n",
    "    if training:\n",
    "        bert_model.train()\n",
    "        torch.set_grad_enabled(True)\n",
    "    else:\n",
    "        bert_model.eval()\n",
    "        torch.set_grad_enabled(False)\n",
    "        \n",
    "    tk0 = tqdm(data_loader, total=int(len(data_loader)))\n",
    "    running_loss = 0.0\n",
    "    counter = 0\n",
    "    counter_correct = 0\n",
    "    for i, batch in enumerate(tk0):\n",
    "        if training:\n",
    "            optim.zero_grad()\n",
    "        \n",
    "        sample = batch[0]  # Assume batch size of 1\n",
    "        loss = torch.tensor(0.0).cuda()\n",
    "        for instance in sample['correct_instances']:\n",
    "            loss1, correct, preds = run_instance(bert_model, instance)\n",
    "            loss += loss1\n",
    "            counter_correct += correct\n",
    "            counter += 1\n",
    "        \n",
    "        num_incorrect = len(sample['incorrect_instances'])\n",
    "        for instance in random.sample(sample['incorrect_instances'], min(10, num_incorrect)):\n",
    "            loss1, correct, preds = run_instance(bert_model, instance)\n",
    "            loss += loss1\n",
    "            counter_correct += correct\n",
    "            counter += 1\n",
    "            \n",
    "            \n",
    "            \n",
    "        if training:\n",
    "            loss.backward()\n",
    "            optim.step()          \n",
    "        running_loss += loss.item()\n",
    "        tk0.set_postfix(loss=(running_loss/counter),\n",
    "                        acc=(counter_correct/counter))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Smarter (than below) rerank algo that does a stable sort with a threshold value\n",
    "\n",
    "\n",
    "import functools\n",
    "def cmp(x, y):\n",
    "    \"\"\"\n",
    "    Replacement for built-in function cmp that was removed in Python 3\n",
    "\n",
    "    Compare the two objects x and y and return an integer according to\n",
    "    the outcome. The return value is negative if x < y, zero if x == y\n",
    "    and strictly positive if x > y.\n",
    "    \"\"\"\n",
    "\n",
    "    return (x > y) - (x < y)\n",
    "\n",
    "def compare(thresh, a, b):\n",
    "    a = a['score']\n",
    "    b = b['score'] + thresh\n",
    "    return cmp(a, b)\n",
    "    \n",
    "\n",
    "def run_rerank(bert_model, data_loader, thresh, print_errors=False):\n",
    "    bert_model.eval()\n",
    "    torch.set_grad_enabled(False)\n",
    "    \n",
    "    tk0 = tqdm(data_loader, total=int(len(data_loader)))\n",
    "    counter = 0\n",
    "    counter_rerank_correct = 0\n",
    "    counter_original_correct = 0\n",
    "    \n",
    "    compare_f = functools.partial(compare, thresh)\n",
    "    \n",
    "    for i, batch in enumerate(tk0):\n",
    "        sample = batch[0]  # Assume batch size of 1\n",
    "        counter += 1\n",
    "        instances = sample['instances']\n",
    "        original_correct = False\n",
    "        if instances[0]['target'] > 0.5:\n",
    "            original_correct = True\n",
    "            counter_original_correct += 1\n",
    "        \n",
    "        preds = []\n",
    "        myinstances = []\n",
    "        for instance in instances[:10]:\n",
    "            instance = instance.copy()\n",
    "            loss, correct, pred = run_instance(bert_model, instance)\n",
    "            preds.append(pred[0][1].item()) # Remove batch, get prob of 1\n",
    "            instance['score'] = pred[0][1].item()\n",
    "            myinstances.append(instance)\n",
    "            \n",
    "        pred_instance = list(reversed(sorted(myinstances, key=functools.cmp_to_key(compare_f))))[0]\n",
    "        rerank_correct = False\n",
    "        \n",
    "        if pred_instance['target'] > 0.5:\n",
    "            rerank_correct = True\n",
    "            counter_rerank_correct += 1\n",
    "        \n",
    "        if print_errors and original_correct and not rerank_correct:\n",
    "            print('\\n')\n",
    "            print('*' * 10)\n",
    "            print(sample['utterance'])\n",
    "            for index, ins in enumerate(sample['instances'][:10]):\n",
    "                p = False\n",
    "                if index == 0:\n",
    "                    print('=' * 10)\n",
    "                    print(f'CORRECT QUERY ; score: {ins[\"score\"]:.2f}')\n",
    "                    p = True\n",
    "                if ins == pred_instance:\n",
    "                    print('=' * 10)\n",
    "                    print(f'SELECTED QUERY ; score: {ins[\"score\"]:.2f}')\n",
    "                    p = True\n",
    "                if p:\n",
    "                    print(ins['sql_query'])\n",
    "                    print(ins['columns_used'])\n",
    "                    print(ins['tables_used'])\n",
    "            \n",
    "        rerank_acc = counter_rerank_correct/counter\n",
    "        tk0.set_postfix(original_acc=(counter_original_correct/counter),\n",
    "                        rerank_acc=rerank_acc)\n",
    "    return rerank_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simpler reranking that only compares the best with the top and accepts if it exceeds a threshold\n",
    "# The one above seems to perform 0.3% better so we'll use that\n",
    "# def run_rerank(bert_model, data_loader, thresh, print_errors=False):\n",
    "#     bert_model.eval()\n",
    "#     torch.set_grad_enabled(False)\n",
    "    \n",
    "#     tk0 = tqdm(data_loader, total=int(len(data_loader)))\n",
    "#     counter = 0\n",
    "#     counter_rerank_correct = 0\n",
    "#     counter_original_correct = 0\n",
    "    \n",
    "#     for i, batch in enumerate(tk0):\n",
    "#         sample = batch[0]  # Assume batch size of 1\n",
    "#         counter += 1\n",
    "#         instances = sample['instances']\n",
    "#         original_correct = False\n",
    "#         if instances[0]['target'] > 0.5:\n",
    "#             original_correct = True\n",
    "#             counter_original_correct += 1\n",
    "        \n",
    "#         preds = []\n",
    "#         for instance in instances[:10]:\n",
    "#             loss, correct, pred = run_instance(bert_model, instance)\n",
    "#             preds.append(pred[0][1].item()) # Remove batch, get prob of 1\n",
    "#         best_pred_index = preds.index(max(preds))  # Get the index of the max, picking the first one if there's a conflict\n",
    "#         rerank_correct = False\n",
    "#         if preds[best_pred_index] - preds[0] < thresh:\n",
    "#             # We're only going to rerank if there's a large discrepancy\n",
    "#             best_pred_index = 0\n",
    "        \n",
    "#         if instances[best_pred_index]['target'] > 0.5:\n",
    "#             rerank_correct = True\n",
    "#             counter_rerank_correct += 1\n",
    "        \n",
    "#         if print_errors and original_correct and not rerank_correct:\n",
    "#             print('\\n')\n",
    "#             print('*' * 10)\n",
    "#             print(sample['utterance'])\n",
    "#             for index, ins in enumerate(sample['instances'][:10]):\n",
    "#                 p = False\n",
    "#                 if index == 0:\n",
    "#                     print('=' * 10)\n",
    "#                     print(f'CORRECT QUERY ; score: {preds[index]}')\n",
    "#                     p = True\n",
    "#                 if index == best_pred_index:\n",
    "#                     print('=' * 10)\n",
    "#                     print(f'SELECTED QUERY ; score: {preds[index]}')\n",
    "#                     p = True\n",
    "#                 if p:\n",
    "#                     print(ins['sql_query'])\n",
    "#                     print(ins['columns_used'])\n",
    "#                     print(ins['tables_used'])\n",
    "            \n",
    "#         rerank_acc = counter_rerank_correct/counter\n",
    "#         tk0.set_postfix(original_acc=(counter_original_correct/counter),\n",
    "#                         rerank_acc=rerank_acc)\n",
    "#     return rerank_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-28T04:42:36.589034Z",
     "start_time": "2019-09-28T04:42:36.551451Z"
    }
   },
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "\n",
    "print('Initial validation run')\n",
    "best_rerank_acc = run_rerank(bert_model, val_loader, thresh=0.1, print_errors=False)\n",
    "run_epoch(bert_model, val_loader, train_bert, optim, training=False, print_samples=False)\n",
    "best_epoch = -1\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "    print('-' * 10)\n",
    "    \n",
    "    run_epoch(bert_model, train_loader, train_bert, optim, training=True)\n",
    "\n",
    "    print('Validation')\n",
    "    \n",
    "    run_epoch(bert_model, val_loader, train_bert, optim, training=False)\n",
    "    \n",
    "    print('Rerank')\n",
    "    \n",
    "    rerank_acc = run_rerank(bert_model, val_loader, thresh=0.1, print_errors=False)\n",
    "    if rerank_acc > best_rerank_acc:\n",
    "        best_epoch = epoch\n",
    "        best_rerank_acc = rerank_acc\n",
    "        print(f'Saving model at epoch {epoch}')\n",
    "        torch.save(bert_model, 'best_disrim_model.pth')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(bert_model, 'final_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bert_model.load_state_dict(torch.load('bert_model.pth'))\n",
    "with open('best_discrim_model_full_spider_retrained.pth', 'rb') as f:\n",
    "    #file = f.read()\n",
    "    bert_model = torch.load(f)\n",
    "bert_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(bert_model.state_dict(), 'best_discrim_model_full_spider_retrained.state.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_rerank(bert_model, val_normaleval_loader, thresh=0.10, print_errors=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "54.8 (harder eval) vs 55.4 (normal eval)  # bert + gnn + train_spider + HIGH LR\n",
    "\n",
    "50.1/54.5 (harder eval) vs 51.2/55.7 (normal eval) # non retrained reranker + bert + gnn + train_all\n",
    "\n",
    "50.1/54.5 (harder eval) vs 51.2/56.3 (normal eval) # retrained ranker + bert + gnn + train_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
